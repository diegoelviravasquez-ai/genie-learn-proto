"""
GENIE Learn â€” Test de IntegraciÃ³n End-to-End
==============================================
Verifica que todos los mÃ³dulos se conectan correctamente
y que el middleware pedagÃ³gico produce comportamientos
verificablemente distintos segÃºn la configuraciÃ³n docente.

EjecuciÃ³n: python test_integration.py
"""

import sys
import os
import time

# Forzar modo sin API key
os.environ.pop("OPENAI_API_KEY", None)
os.environ.pop("ANTHROPIC_API_KEY", None)

from middleware import PedagogicalMiddleware, PedagogicalConfig
from rag_pipeline import get_rag_pipeline, SAMPLE_COURSE_CONTENT
from llm_client import get_llm_client
from cognitive_analyzer import CognitiveAnalyzer, EngagementProfiler, BLOOM_LEVELS
from trust_dynamics import TrustDynamicsAnalyzer


def header(title: str):
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}")


def test_rag_pipeline():
    header("TEST 1: RAG Pipeline")
    rag = get_rag_pipeline(use_openai=False)
    n = rag.ingest_text(SAMPLE_COURSE_CONTENT, "Fundamentos_Programacion.pdf")
    print(f"  âœ“ Ingested {n} chunks")

    results = rag.retrieve("Â¿QuÃ© es un bucle for?", top_k=3)
    print(f"  âœ“ Retrieved {len(results)} chunks for 'bucle for'")
    for r in results:
        print(f"    - [{r['source']}] score={r['score']:.3f} | {r['text'][:60]}...")

    context = rag.build_context("recursiÃ³n", top_k=2)
    assert "recursiÃ³n" in context.lower() or "caso base" in context.lower()
    print(f"  âœ“ Context for 'recursiÃ³n': {len(context)} chars")

    stats = rag.get_stats()
    print(f"  âœ“ Stats: {stats}")
    return rag


def test_scaffolding_escalation():
    header("TEST 2: Scaffolding SocrÃ¡tico â€” EscalaciÃ³n progresiva")
    config = PedagogicalConfig(scaffolding_mode="socratic")
    mw = PedagogicalMiddleware(config)
    llm = get_llm_client()
    sid = "est_socratic"

    expected_keywords = ["SOCRÃTICO", "PISTA", "EJEMPLO", "EXPLICACIÃ“N"]

    for i in range(7):
        pre = mw.pre_process(sid, f"Pregunta {i+1} sobre bucles for")
        result = llm.chat(pre["system_prompt"], f"Pregunta {i+1}", "")
        post = mw.post_process(sid, result["response"])

        level = pre["scaffolding_level"]
        label = expected_keywords[min(level, 3)]
        in_prompt = label in pre["system_prompt"]
        print(f"  InteracciÃ³n {i+1}: level={level} ({label}) | "
              f"en system_prompt={'âœ“' if in_prompt else 'âœ—'}")

    # Verificar que se llegÃ³ al nivel 3
    final_state = mw.conversation_states[sid]
    print(f"  âœ“ Estado final: level={final_state['level']}, attempts={final_state['attempts']}")
    assert final_state["level"] >= 2, "Should have escalated to at least level 2"


def test_direct_mode_vs_socratic():
    header("TEST 3: Modo directo vs SocrÃ¡tico â€” Verificablemente distintos")

    prompt = "Â¿CÃ³mo funciona un bucle while?"

    # Socratic
    config_s = PedagogicalConfig(scaffolding_mode="socratic")
    mw_s = PedagogicalMiddleware(config_s)
    pre_s = mw_s.pre_process("est_s", prompt)

    # Direct
    config_d = PedagogicalConfig(scaffolding_mode="direct")
    mw_d = PedagogicalMiddleware(config_d)
    pre_d = mw_d.pre_process("est_d", prompt)

    print(f"  SocrÃ¡tico system_prompt ({len(pre_s['system_prompt'])} chars):")
    print(f"    '{pre_s['system_prompt'][:120]}...'")
    print(f"  Directo system_prompt ({len(pre_d['system_prompt'])} chars):")
    print(f"    '{pre_d['system_prompt'][:120]}...'")

    assert "SOCRÃTICO" in pre_s["system_prompt"]
    assert "SOCRÃTICO" not in pre_d["system_prompt"]
    print(f"  âœ“ Los system prompts son verificablemente distintos")

    # Verify mock LLM responds differently
    llm = get_llm_client()
    r_s = llm.chat(pre_s["system_prompt"], prompt, "")
    r_d = llm.chat(pre_d["system_prompt"], prompt, "")
    print(f"  SocrÃ¡tico response: '{r_s['response'][:80]}...'")
    print(f"  Directo response:   '{r_d['response'][:80]}...'")
    assert r_s["response"] != r_d["response"], "Responses should differ"
    print(f"  âœ“ Las respuestas del LLM son diferentes segÃºn el modo")


def test_daily_limit():
    header("TEST 4: LÃ­mite diario de prompts")
    config = PedagogicalConfig(max_daily_prompts=3)
    mw = PedagogicalMiddleware(config)
    sid = "est_limit"

    for i in range(4):
        pre = mw.pre_process(sid, f"Pregunta {i+1}")
        status = "âœ“ allowed" if pre["allowed"] else f"â›” blocked: {pre['block_reason'][:50]}"
        print(f"  Prompt {i+1}/3: {status}")

    pre_final = mw.pre_process(sid, "Una mÃ¡s")
    assert not pre_final["allowed"], "Should be blocked"
    print(f"  âœ“ LÃ­mite funciona correctamente")


def test_hallucination_injection():
    header("TEST 5: AlucinaciÃ³n pedagÃ³gica controlada")

    # 100% injection rate
    config = PedagogicalConfig(forced_hallucination_pct=1.0)
    mw = PedagogicalMiddleware(config)
    post = mw.post_process("est_h", "Los arrays empiezan en el Ã­ndice 0.")
    assert post["hallucination_injected"]
    assert "NOTA PEDAGÃ“GICA" in post["response"]
    print(f"  âœ“ 100% rate: inyecciÃ³n correcta")
    print(f"    Aviso: '...{post['response'][-80:]}'")

    # 0% injection rate
    config0 = PedagogicalConfig(forced_hallucination_pct=0.0)
    mw0 = PedagogicalMiddleware(config0)
    post0 = mw0.post_process("est_h0", "Los arrays empiezan en el Ã­ndice 0.")
    assert not post0["hallucination_injected"]
    print(f"  âœ“ 0% rate: sin inyecciÃ³n")


def test_copy_paste_detection():
    header("TEST 6: DetecciÃ³n de copy-paste")
    config = PedagogicalConfig()
    mw = PedagogicalMiddleware(config)

    organic = "Â¿cÃ³mo hago un for que recorra un array?"
    pasted = (
        "Ejercicio 3.2: Dado el siguiente array de enteros, implementar un mÃ©todo "
        "que recorra todos los elementos y calcule la media aritmÃ©tica. Se pide "
        "ademÃ¡s que el programa detecte si algÃºn valor supera el doble de la media."
    )

    pre_org = mw.pre_process("est_cp1", organic)
    pre_cp = mw.pre_process("est_cp2", pasted)

    print(f"  OrgÃ¡nico: score={pre_org['copy_paste_score']:.2f}")
    print(f"  Pegado:   score={pre_cp['copy_paste_score']:.2f}")
    assert pre_cp["copy_paste_score"] > pre_org["copy_paste_score"]
    print(f"  âœ“ El texto pegado tiene mayor score que el orgÃ¡nico")


def test_cognitive_analysis():
    header("TEST 7: AnÃ¡lisis cognitivo (Bloom + ICAP)")
    analyzer = CognitiveAnalyzer()

    tests = [
        ("Â¿QuÃ© es una variable?", 1, "Recordar"),
        ("Explica cÃ³mo funciona un bucle for", 2, "Comprender"),
        ("Escribe un programa que calcule factorial", 3, "Aplicar"),
        ("Â¿CuÃ¡l es mejor: recursiÃ³n o iteraciÃ³n?", 5, "Evaluar"),
        ("DiseÃ±a un algoritmo nuevo para ordenar", 6, "Crear"),
    ]

    for prompt, expected_level, expected_name in tests:
        result = analyzer.analyze(prompt)
        match = "âœ“" if result.bloom_level == expected_level else "â‰ˆ"
        print(f"  {match} '{prompt[:45]}...' â†’ L{result.bloom_level} "
              f"({result.bloom_name}) ICAP:{result.icap_label} "
              f"conf={result.bloom_confidence:.2f}")


def test_trust_dynamics():
    header("TEST 8: DinÃ¡micas de confianza")
    analyzer = TrustDynamicsAnalyzer()

    tests = [
        ("Â¿EstÃ¡s seguro de que eso es correcto?", "verification"),
        ("Ok perfecto, siguiente pregunta", "sobre-confianza"),
        ("No me sirve, dame la respuesta directa", "frustraciÃ³n"),
        ("A ver si entiendo bien, o sea que...", "reformulaciÃ³n"),
    ]

    for prompt, expected_type in tests:
        signal = analyzer.analyze_prompt("est_trust", prompt)
        print(f"  '{prompt[:45]}...' â†’ type={signal.signal_type} "
              f"direction={signal.trust_direction:+.2f}")


def test_block_direct_solutions():
    header("TEST 9: Bloqueo de soluciones directas")

    config_block = PedagogicalConfig(block_direct_solutions=True)
    mw_block = PedagogicalMiddleware(config_block)
    pre = mw_block.pre_process("est_sol", "ResuÃ©lveme el ejercicio 3")
    assert "NO la proporciones" in pre["system_prompt"] or "NO" in pre["system_prompt"]
    print(f"  âœ“ Con bloqueo: system_prompt incluye instrucciÃ³n de no resolver")

    config_free = PedagogicalConfig(block_direct_solutions=False)
    mw_free = PedagogicalMiddleware(config_free)
    pre_free = mw_free.pre_process("est_sol2", "ResuÃ©lveme el ejercicio 3")
    assert "NO la proporciones" not in pre_free["system_prompt"]
    print(f"  âœ“ Sin bloqueo: system_prompt no restringe soluciones")


def test_topic_detection():
    header("TEST 10: DetecciÃ³n de topics")
    config = PedagogicalConfig()
    mw = PedagogicalMiddleware(config)

    tests = [
        ("Â¿CÃ³mo declaro una variable int?", ["variables"]),
        ("El bucle while no termina", ["bucles", "depuraciÃ³n"]),
        ("Quiero hacer una funciÃ³n recursiva", ["funciones", "recursiÃ³n"]),
    ]

    for prompt, expected in tests:
        pre = mw.pre_process(f"est_topic_{hash(prompt)}", prompt)
        topics = pre["detected_topics"]
        matched = all(e in topics for e in expected)
        print(f"  {'âœ“' if matched else 'âœ—'} '{prompt[:40]}...' â†’ {topics} "
              f"(esperado: {expected})")


def test_full_pipeline():
    header("TEST 11: Pipeline completo end-to-end")

    config = PedagogicalConfig(
        scaffolding_mode="socratic",
        block_direct_solutions=True,
        forced_hallucination_pct=0.0,
        use_rag=True,
        max_daily_prompts=20,
    )
    mw = PedagogicalMiddleware(config)
    rag = get_rag_pipeline(use_openai=False)
    rag.ingest_text(SAMPLE_COURSE_CONTENT, "curso.pdf")
    llm = get_llm_client()
    analyzer = CognitiveAnalyzer()
    trust = TrustDynamicsAnalyzer()

    prompts = [
        "Â¿QuÃ© es un array en Java?",
        "No entiendo, Â¿cÃ³mo declaro uno?",
        "Dame un ejemplo de recorrer array",
        "Ya entendÃ­, ahora explÃ­came el for-each",
    ]

    for i, prompt in enumerate(prompts):
        pre = mw.pre_process("pipeline_test", prompt)
        if not pre["allowed"]:
            print(f"  â›” Blocked: {pre['block_reason']}")
            continue

        context = rag.build_context(prompt, top_k=3) if config.use_rag else ""
        result = llm.chat(pre["system_prompt"], prompt, context)
        post = mw.post_process("pipeline_test", result["response"])
        cognitive = analyzer.analyze(prompt)
        trust_signal = trust.analyze_prompt("pipeline_test", prompt)

        mw.log_interaction(
            student_id="pipeline_test",
            prompt_raw=prompt,
            pre_result=pre,
            response_raw=result["response"],
            post_result=post,
            response_time_ms=result.get("response_time_ms", 200),
        )

        print(f"  [{i+1}] Scaff:L{pre['scaffolding_level']} "
              f"Bloom:L{cognitive.bloom_level}({cognitive.bloom_name}) "
              f"ICAP:{cognitive.icap_label} "
              f"Trust:{trust_signal.signal_type} "
              f"Topics:{pre['detected_topics']}")

    summary = mw.get_analytics_summary()
    print(f"\n  Analytics summary:")
    print(f"    Total: {summary['total_interactions']} interactions")
    print(f"    Scaffolding: {summary['scaffolding_levels']}")
    print(f"    Topics: {summary['topic_distribution']}")
    print(f"  âœ“ Pipeline completo funcional")


if __name__ == "__main__":
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘   GENIE Learn â€” Test de IntegraciÃ³n End-to-End        â•‘")
    print("â•‘   Diego Elvira VÃ¡squez Â· CP25/152 Â· GSIC/EMIC-UVa    â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    tests = [
        test_rag_pipeline,
        test_scaffolding_escalation,
        test_direct_mode_vs_socratic,
        test_daily_limit,
        test_hallucination_injection,
        test_copy_paste_detection,
        test_cognitive_analysis,
        test_trust_dynamics,
        test_block_direct_solutions,
        test_topic_detection,
        test_full_pipeline,
    ]

    passed = 0
    failed = 0
    for test_fn in tests:
        try:
            test_fn()
            passed += 1
        except Exception as e:
            failed += 1
            print(f"  âœ— FAILED: {e}")

    header(f"RESULTADO: {passed}/{passed+failed} tests passed")
    if failed == 0:
        print("  ğŸ‰ TODO FUNCIONA â€” Sistema listo para demo")
    else:
        print(f"  âš ï¸  {failed} tests fallaron")

    sys.exit(0 if failed == 0 else 1)
