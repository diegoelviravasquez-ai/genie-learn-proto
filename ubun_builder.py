"""
ubun_builder.py â€” Minijuego Streamlit: Construye UBUN.IA
==========================================================
Robot educativo que se monta pieza a pieza. Cada pieza = una tecnologÃ­a.
El jugador demuestra comprensiÃ³n respondiendo preguntas â†’ la pieza se instala.

Ejecutar: streamlit run ubun_builder.py --server.port 8505
"""

import streamlit as st
import re

st.set_page_config(
    page_title="Construye UBUN.IA",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="collapsed",
)

# â”€â”€â”€ DefiniciÃ³n de las 10 piezas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PIECES = [
    {
        "id": "python",
        "name": "Esqueleto",
        "emoji": "ğŸ¦´",
        "tech": "Python",
        "analogy": "El esqueleto define la estructura. Sin Ã©l no hay robot.",
        "code": '''# app.py â€” punto de entrada
# from ecosystem_orchestrator import EcosystemOrchestrator
# from middleware import PedagogicalConfig
#
# config = PedagogicalConfig()
# orch = EcosystemOrchestrator(config, rag_pipeline=rag, llm_client=llm)
# result = orch.process_interaction("est_01", "Â¿QuÃ© es recursiÃ³n?")

# Mock para funcionar sin LLM
def mock_response(prompt):
    return f"[Demo] Procesado: {prompt[:40]}"
''',
        "q1": "Â¿Por quÃ© Python y no JavaScript para UBUN.IA?",
        "q2": "Escribe una funciÃ³n que cuente prompts por estudiante (en pseudocÃ³digo).",
        "q3": "Â¿Por quÃ© el orchestrator usa _safe_import en vez de import directo?",
        "keywords": ["python", "estructura", "orchestrator", "middleware", "config", "funciÃ³n", "import"],
        "in_action": "Flujo: app.py â†’ EcosystemOrchestrator â†’ middleware.pre_process â†’ LLM â†’ middleware.post_process",
        "ficha": "Sospechoso desde 1991. Sin coartada para la desapariciÃ³n del JavaScript en proyectos de ML. Conocido por frecuentar entornos cientÃ­ficos.",
    },
    {
        "id": "middleware",
        "name": "CorazÃ³n",
        "emoji": "â¤ï¸",
        "tech": "Middleware",
        "analogy": "El corazÃ³n bombea las reglas pedagÃ³gicas a cada respuesta.",
        "code": '''# middleware.py â€” reglas antes y despuÃ©s del LLM
def pre_process(self, student_id: str, raw_prompt: str) -> dict:
    # LÃ­mite diario, copy-paste, scaffolding level
    return {"allowed": True, "system_prompt": ..., "processed_prompt": ...}

def post_process(self, student_id: str, response: str) -> dict:
    # Truncado, alucinaciÃ³n pedagÃ³gica
    return {"response": response}''',
        "q1": "Â¿QuÃ© hace el middleware ANTES de llamar al LLM?",
        "q2": "Â¿CÃ³mo activarÃ­as el modo socrÃ¡tico en el config?",
        "q3": "Â¿QuÃ© pasarÃ­a si el middleware no existiera?",
        "keywords": ["middleware", "pre_process", "post_process", "scaffolding", "socrÃ¡tico", "reglas"],
        "in_action": "Los 8 modos de scaffolding: socratic, hints, examples, analogies, direct, challenge, rubber_duck, progressive",
        "ficha": "No tiene cara propia. Intercepta toda comunicaciÃ³n antes de que llegue al destinatario. La policÃ­a lo considera 'de interÃ©s' en todos los casos.",
    },
    {
        "id": "llm",
        "name": "Cerebro",
        "emoji": "ğŸ§ ",
        "tech": "LLM API",
        "analogy": "El cerebro genera lenguaje. Pero no decide â€” obedece al corazÃ³n.",
        "code": '''# llm_client.py â€” Anthropic / OpenAI
def chat(self, system_prompt: str, user_prompt: str, context: str = "") -> dict:
    # system_prompt lleva las instrucciones pedagÃ³gicas del middleware
    response = self.client.messages.create(
        model="claude-sonnet",
        system=system_prompt,
        messages=[{"role": "user", "content": user_prompt + "\\n\\nContexto: " + context}]
    )
    return {"response": response.content[0].text}''',
        "q1": "Â¿QuÃ© recibe el LLM ademÃ¡s del mensaje del estudiante?",
        "q2": "Â¿CÃ³mo se pasa el modo socrÃ¡tico al modelo?",
        "q3": "Â¿Por quÃ© el LLM no debe decidir solo el nivel de ayuda?",
        "keywords": ["llm", "system_prompt", "contexto", "anthropic", "openai", "modelo", "pedagÃ³gico"],
        "in_action": "Llamada a Anthropic con system_prompt que incluye nivel de scaffolding y rol de tutor",
        "ficha": "Extranjero. Ha leÃ­do todo internet pero jura que no recuerda nada especÃ­fico. Responde preguntas con una fluidez que resulta sospechosa.",
    },
    {
        "id": "rag",
        "name": "Memoria",
        "emoji": "ğŸ’¾",
        "tech": "ChromaDB/RAG",
        "analogy": "La memoria guarda los apuntes del curso. El cerebro solo recuerda lo que le pasas.",
        "code": '''# rag_pipeline.py â€” retrieval
def retrieve(self, query: str, top_k: int = 3) -> list[dict]:
    # Embeddings del query vs chunks del curso
    results = self.collection.query(query_texts=[query], n_results=top_k)
    return [{"text": doc, "source": meta["source"]} for doc, meta in ...]

# El contexto se inyecta en el prompt al LLM
context = "\\n\\n".join(chunk["text"] for chunk in chunks)''',
        "q1": "Â¿QuÃ© es RAG y por quÃ© lo usa GENIE?",
        "q2": "Â¿CÃ³mo se usa el resultado de retrieve() en la llamada al LLM?",
        "q3": "Â¿Por quÃ© top_k=3 y no 10 chunks?",
        "keywords": ["rag", "retrieve", "chunks", "contexto", "embedding", "chroma", "curso"],
        "in_action": "rag.retrieve(prompt, top_k=3) devuelve los 3 fragmentos mÃ¡s relevantes del material del curso",
        "ficha": "Especialista en recuperar lo que otros olvidaron. Opera a travÃ©s de ChromaDB. No deja huellas â€”solo vectores de 1536 dimensiones.",
    },
    {
        "id": "pandas",
        "name": "MÃºsculos",
        "emoji": "ğŸ’ª",
        "tech": "Pandas",
        "analogy": "Los mÃºsculos procesan los datos. Sin Pandas el dashboard no podrÃ­a agrupar ni calcular.",
        "code": '''# analytics/bridge.py â€” perfiles por estudiante
def get_student_profiles(course_id=None) -> pd.DataFrame:
    df = get_interactions_df(course_id=course_id)
    agg = df.groupby("student_id").agg(
        total_prompts=("id", "count"),
        bloom_mean=("bloom_level", "mean"),
        autonomy_score=("copy_paste_score", lambda s: 1 - s.mean()),
    ).reset_index()
    return agg''',
        "q1": "Â¿QuÃ© es un DataFrame y por quÃ© lo necesita el dashboard?",
        "q2": "Â¿CÃ³mo calcularÃ­as el bloom_mean por student_id?",
        "q3": "Â¿Por quÃ© groupby es mejor que un bucle for aquÃ­?",
        "keywords": ["pandas", "dataframe", "groupby", "bloom", "estudiante", "agregar"],
        "in_action": "get_student_profiles() agrupa por student_id y devuelve DataFrame con bloom_mean, autonomy_score",
        "ficha": "Nombre en clave: DataFrame. Agrupa, filtra y calcula sin mostrar emociÃ³n. Visto en compaÃ±Ã­a de NumPy en horas intempestivas.",
    },
    {
        "id": "plotly",
        "name": "Ojos",
        "emoji": "ğŸ‘ï¸",
        "tech": "Plotly",
        "analogy": "Los ojos visualizan el aprendizaje. Scatter, heatmaps y radares son la vista del robot.",
        "code": '''# dashboard/demo_completo.py â€” scatter de cuadrantes
fig = go.Figure()
fig.add_trace(go.Scatter(
    x=df["autonomy_score"], y=df["bloom_mean"],
    mode="markers", marker=dict(size=df["total_prompts"]*2),
    text=df["name"]
))
fig.add_vline(x=0.5); fig.add_hline(y=3)  # cuadrantes
st.plotly_chart(fig)''',
        "q1": "Â¿Para quÃ© sirve Plotly en un dashboard de LA?",
        "q2": "Â¿CÃ³mo representarÃ­as autonomÃ­a vs Bloom en un solo grÃ¡fico?",
        "q3": "Â¿Por quÃ© aÃ±adir lÃ­neas en x=0.5 e y=3 en el scatter?",
        "keywords": ["plotly", "scatter", "grÃ¡fico", "visualizaciÃ³n", "bloom", "autonomÃ­a", "cuadrante"],
        "in_action": "Scatter autonomy vs bloom_mean en demo_completo.py con cuadrantes y ZONA DE RIESGO",
        "ficha": "Hace que los datos actÃºen. Testigos describen grÃ¡ficos que 'responden' cuando los tocas. Antecedentes: Florence Nightingale, 1858.",
    },
    {
        "id": "streamlit",
        "name": "Manos",
        "emoji": "ğŸ¤",
        "tech": "Streamlit",
        "analogy": "Las manos son la interfaz. El docente toca sliders, el estudiante escribe en el chat.",
        "code": '''# app.py â€” vistas
view = st.radio("Vista", ["Estudiante", "Docente â€” Config", "Docente â€” Analytics",
                          "Mapa EpistÃ©mico", "Demo en Vivo", "Investigador"])

if view == "Estudiante":
    prompt = st.chat_input("Escribe tu pregunta...")
    if prompt:
        result = orch.process_interaction(student_id, prompt)
        st.chat_message("assistant").markdown(result.response_text)''',
        "q1": "Â¿QuÃ© es Streamlit y por quÃ© se usa en GENIE?",
        "q2": "Â¿CÃ³mo se cambia de vista Estudiante a Docente en app.py?",
        "q3": "Â¿Por quÃ© usar st.session_state para el historial de chat?",
        "keywords": ["streamlit", "vista", "chat", "st.", "session_state", "interfaz"],
        "in_action": "Las 7 vistas de app.py: Estudiante, PrÃ¡ctica Guiada, Docente Config, Docente Analytics, Mapa EpistÃ©mico, Demo, Investigador",
        "ficha": "Construye interfaces sin JavaScript. Los puristas del frontend lo consideran una afrenta. Los cientÃ­ficos de datos, una salvaciÃ³n.",
    },
    {
        "id": "postgresql",
        "name": "EstÃ³mago",
        "emoji": "ğŸ—„ï¸",
        "tech": "PostgreSQL",
        "analogy": "El estÃ³mago guarda todo. Interacciones, estudiantes y configs viven en tablas.",
        "code": '''# data/models.py â€” SQLAlchemy
class Interaction(Base):
    __tablename__ = "interactions"
    id = Column(Integer, primary_key=True)
    student_id = Column(Integer, ForeignKey("students.id"))
    prompt = Column(Text); response = Column(Text)
    bloom_level = Column(Integer)
    copy_paste_score = Column(Float)
    scaffolding_mode = Column(String(64))
    timestamp = Column(DateTime)''',
        "q1": "Â¿Por quÃ© guardar las interacciones en una base de datos?",
        "q2": "Â¿QuÃ© columnas tiene la tabla Interaction en el proyecto?",
        "q3": "Â¿CuÃ¡ndo se llama a log_interaction en el orchestrator?",
        "keywords": ["postgresql", "tabla", "interaction", "student", "bloom", "log_interaction"],
        "in_action": "Tabla Interaction: student_id, prompt, response, bloom_level, copy_paste_score, scaffolding_mode, timestamp",
        "ficha": "El archivero. Recuerda todo. Para siempre. Sin opiniÃ³n sobre el contenido. Trabaja en SQL, un idioma que nadie eligiÃ³ pero todo el mundo usa.",
    },
    {
        "id": "fastapi",
        "name": "Sistema nervioso",
        "emoji": "ğŸ¦·",
        "tech": "FastAPI",
        "analogy": "El sistema nervioso conecta los mÃ³dulos. La API expone el chatbot a Moodle o a otros clientes.",
        "code": '''# api.py â€” endpoint de chat
@app.post("/chat")
def chat(request: ChatRequest):
    pre = middleware.pre_process(request.student_id, request.prompt)
    if not pre["allowed"]:
        raise HTTPException(403, pre["block_reason"])
    response = llm.chat(pre["system_prompt"], pre["processed_prompt"], context)
    return {"response": response, "scaffolding_level": pre["scaffolding_level"]}''',
        "q1": "Â¿Para quÃ© sirve FastAPI en GENIE?",
        "q2": "Â¿QuÃ© devuelve el endpoint POST /chat?",
        "q3": "Â¿Por quÃ© el middleware se llama antes del LLM en la API?",
        "keywords": ["fastapi", "api", "endpoint", "post", "chat", "middleware"],
        "in_action": "Endpoint POST /chat recibe student_id y prompt, devuelve response y scaffolding_level",
        "ficha": "IntÃ©rprete entre sistemas que no se hablan. Transforma peticiones HTTP en acciones Python. Completamente ajeno a las consecuencias.",
    },
    {
        "id": "git",
        "name": "ADN",
        "emoji": "ğŸ“¦",
        "tech": "Git",
        "analogy": "El ADN es la memoria del proyecto. Cada commit guarda una versiÃ³n; el PR revisa el cambio.",
        "code": '''# Flujo tÃ­pico en GENIE Learn
git checkout -b feature/nueva-vista
# ... editar app.py, aÃ±adir vista ...
git add app.py
git commit -m "feat: vista PrÃ¡ctica Guiada con mini-retos"
git push origin feature/nueva-vista
# Abrir Pull Request â†’ revisiÃ³n â†’ merge a main''',
        "q1": "Â¿Por quÃ© usar Git en un proyecto de investigaciÃ³n con cÃ³digo?",
        "q2": "Â¿QuÃ© es un branch y cuÃ¡ndo crearlo?",
        "q3": "Â¿Por quÃ© hacer commit de app.py y middleware por separado si cambian juntos?",
        "keywords": ["git", "commit", "branch", "merge", "versiÃ³n", "pr", "pull request"],
        "in_action": "Flujo: branch â†’ commit de mÃ³dulos â†’ push â†’ Pull Request â†’ merge a main",
        "ficha": "Testigo perfecto. Recuerda quÃ©, cuÃ¡ndo y quiÃ©n. El por quÃ© depende del mensaje de commit, que rara vez es suficientemente explicativo.",
    },
]

COLOR_INSTALLED = "#048A81"
COLOR_CURRENT = "#E87722"
COLOR_LOCKED = "#444"

# â”€â”€â”€ Estilos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.markdown(
    f"""
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap');
    html, body, [class*="css"] {{ font-family: 'Inter', sans-serif; background: #0a0a0a; color: #eee; }}
    .stApp {{ background: linear-gradient(180deg, #0a0a0a 0%, #0d0d0d 100%); }}
    .ubun-title {{ font-size: 2rem; font-weight: 600; margin-bottom: 4px; }}
    .ubun-sub {{ color: #888; margin-bottom: 24px; }}
    .ubun-robot {{ font-family: 'Consolas', monospace; font-size: 14px; line-height: 1.4; padding: 16px; background: #111; border-radius: 12px; white-space: pre; }}
    .ubun-piece-ok {{ color: {COLOR_INSTALLED}; }}
    .ubun-piece-current {{ color: {COLOR_CURRENT}; animation: pulse 1.5s ease-in-out infinite; }}
    @keyframes pulse {{ 0%,100% {{ opacity: 1; }} 50% {{ opacity: 0.6; }} }}
    .ubun-piece-locked {{ color: {COLOR_LOCKED}; }}
    .ubun-badge {{ display: inline-block; padding: 6px 12px; border-radius: 8px; font-size: 0.9rem; margin-top: 8px; }}
    .ubun-card {{ background: #1a1a1a; border-radius: 12px; padding: 16px; margin-top: 16px; border-left: 4px solid {COLOR_INSTALLED}; }}
    .footer-ubun {{ margin-top: 32px; font-size: 0.8rem; color: #555; }}
    </style>
    """,
    unsafe_allow_html=True,
)

# â”€â”€â”€ Session state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if "ubun_pieces_installed" not in st.session_state:
    st.session_state.ubun_pieces_installed = []
if "ubun_current_piece" not in st.session_state:
    st.session_state.ubun_current_piece = 0
if "ubun_questions_answered" not in st.session_state:
    st.session_state.ubun_questions_answered = {}  # piece_id -> [bool, bool, bool]
if "ubun_answers" not in st.session_state:
    st.session_state.ubun_answers = {}  # piece_id -> [str, str, str]
if "ubun_bloom_history" not in st.session_state:
    st.session_state.ubun_bloom_history = []
if "ubun_just_installed" not in st.session_state:
    st.session_state.ubun_just_installed = None  # piece_id or None
if "ubun_tutorial_done" not in st.session_state:
    st.session_state.ubun_tutorial_done = False

for p in PIECES:
    if p["id"] not in st.session_state.ubun_questions_answered:
        st.session_state.ubun_questions_answered[p["id"]] = [False, False, False]
    if p["id"] not in st.session_state.ubun_answers:
        st.session_state.ubun_answers[p["id"]] = ["", "", ""]

# â”€â”€â”€ Pantalla de bienvenida (tutorial inicial) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if not st.session_state.ubun_tutorial_done:
    st.markdown("""
    <div style="background:#0d0d0d; border:2px solid #C4922A;
                border-radius:12px; padding:32px; max-width:700px;
                margin:40px auto; font-family:monospace;">
      <div style="font-size:2rem; text-align:center; margin-bottom:16px">ğŸ”</div>
      <h2 style="color:#C4922A; text-align:center; margin-bottom:8px">
        EL CASO DE UBUN.IA
      </h2>
      <p style="color:#9A8C78; text-align:center; font-style:italic; margin-bottom:24px">
        Diez piezas desaparecidas. Diez tecnologÃ­as sospechosas.<br>
        Un detective con Python.
      </p>
      <div style="color:#D4CAB8; line-height:1.8; margin-bottom:24px">
        <p>ğŸ¤– <strong style="color:#C4922A">UBUN.IA</strong> es un robot educativo
        desmontado. Cada pieza es una tecnologÃ­a del proyecto GENIE Learn.</p>
        <p>ğŸ” Tu misiÃ³n: <strong style="color:#C4922A">fichar a los 10 sospechosos</strong>
        demostrando que los entiendes.</p>
        <p>ğŸ“‹ <strong style="color:#C4922A">CÃ³mo funciona:</strong></p>
        <ul style="color:#9A8C78; margin-left:20px">
          <li>La columna izquierda muestra el pipeline del robot</li>
          <li>La columna derecha tiene 3 preguntas por pieza</li>
          <li>Las preguntas escalan: Comprender â†’ Aplicar â†’ Analizar</li>
          <li>Responde con mÃ¡s de 20 palabras usando el concepto clave</li>
          <li>Cuando las 3 estÃ©n aceptadas, aparece el botÃ³n "FICHAR SOSPECHOSO"</li>
        </ul>
        <p>ğŸ–ï¸ Tu rango sube conforme fichÃ¡s mÃ¡s tecnologÃ­as.</p>
      </div>
    </div>
    """, unsafe_allow_html=True)

    col1, col2, col3 = st.columns([2, 2, 2])
    with col2:
        if st.button("ğŸ” COMENZAR LA INVESTIGACIÃ“N", type="primary", use_container_width=True):
            st.session_state.ubun_tutorial_done = True
            st.rerun()
    st.stop()

def word_count(text):
    return len((text or "").split())


def answer_acceptable(text: str, keywords: list) -> tuple[bool, str]:
    if word_count(text) < 20:
        return False, "Escribe al menos 20 palabras."
    lower = (text or "").lower()
    for k in keywords:
        if k.lower() in lower:
            return True, "âœ… Respuesta aceptada."
    return False, f"ğŸ”„ Intenta mencionar alguna de: {', '.join(keywords[:4])}..."


# Orden del pipeline vertical (flujo del sistema)
PIPELINE_ORDER = [
    ("rag", "ğŸ’¾", "ChromaDB/RAG", "La memoria guarda los apuntes"),
    ("llm", "ğŸ§ ", "LLM API", "El cerebro genera texto"),
    ("middleware", "â¤ï¸", "Middleware", "El corazÃ³n aplica reglas"),
    ("python", "ğŸ¦´", "Python", "El esqueleto conecta todo"),
    ("pandas", "ğŸ’ª", "Pandas", "Los mÃºsculos procesan datos"),
    ("plotly", "ğŸ‘ï¸", "Plotly", "Los ojos visualizan el aprendizaje"),
    ("streamlit", "ğŸ¤", "Streamlit", "Las manos son la interfaz"),
    ("postgresql", "ğŸ—„ï¸", "PostgreSQL", "El estÃ³mago guarda todo"),
    ("fastapi", "ğŸ¦·", "FastAPI", "El sistema nervioso conecta los mÃ³dulos"),
    ("git", "ğŸ“¦", "Git", "El ADN versiona el proyecto"),
]
ARROW_TEXTS = [
    "busca en apuntes",
    "prompt + contexto RAG",
    "reglas pedagÃ³gicas aplicadas",
    "logs de interacciones",
    "DataFrame de mÃ©tricas",
    "grÃ¡ficos interactivos",
    "persistencia",
    "datos del dashboard",
    "versiona el pipeline",
]


def render_robot(installed, current):
    """Dibuja el robot como pipeline vertical con flechas entre piezas."""
    installed = set(installed)

    def piece_box(pid, emoji, name, role):
        if pid in installed:
            bg = "#0a2a20"
            border = "#048A81"
            op = "1"
            extra = ""
        elif pid == current:
            bg = "#2a1500"
            border = "#E87722"
            op = "1"
            extra = '<div style="font-size:0.55rem;color:#E87722;margin-top:2px;">âš¡ INSTALANDO</div>'
        else:
            bg = "#111"
            border = "#333"
            op = "0.4"
            extra = ""
        return (
            f'<div style="background:{bg};border:2px solid {border};border-radius:8px;'
            f'padding:10px 14px;margin:0 auto;opacity:{op};'
            f'font-family:monospace;text-align:center;max-width:280px;">'
            f'<div style="font-size:1.8rem">{emoji}</div>'
            f'<div style="font-size:0.75rem;color:#eee">{name}</div>'
            f'<div style="font-size:0.6rem;color:#aaa;margin-top:2px;">{role}</div>'
            f'{extra}</div>'
        )

    def arrow_line(text, from_installed, to_installed):
        if from_installed and to_installed:
            color = "#048A81"
            style = "solid"
        else:
            color = "#555"
            style = "dashed"
        return (
            f'<div style="text-align:center;margin:4px 0;">'
            f'<div style="border-left:2px {style} {color};height:20px;margin:0 auto;width:0;"></div>'
            f'<div style="font-size:0.6rem;color:{color};margin:2px 0;">â†“ {text}</div>'
            f'<div style="border-left:2px {style} {color};height:12px;margin:0 auto;width:0;"></div>'
            f'</div>'
        )

    parts = []
    for i, (pid, emoji, name, role) in enumerate(PIPELINE_ORDER):
        parts.append(piece_box(pid, emoji, name, role))
        if i < len(ARROW_TEXTS):
            from_ok = pid in installed
            next_pid = PIPELINE_ORDER[i + 1][0]
            to_ok = next_pid in installed
            parts.append(arrow_line(ARROW_TEXTS[i], from_ok, to_ok))

    html = f"""
    <div style="background:#0d0d0d;border-radius:12px;padding:20px;
                border:1px solid #1a1a2e;">
      <div style="display:flex;flex-direction:column;align-items:center;">
        {"".join(parts)}
      </div>
    </div>
    """
    st.markdown(html, unsafe_allow_html=True)


# â”€â”€â”€ TÃ­tulo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.markdown('<p class="ubun-title">ğŸ” EL CASO DE UBUN.IA</p>', unsafe_allow_html=True)
st.markdown('<p class="ubun-sub">Diez piezas desaparecidas. Diez tecnologÃ­as sospechosas.<br>Un detective con Python.</p>', unsafe_allow_html=True)

n_installed = len(st.session_state.ubun_pieces_installed)
idx = min(st.session_state.ubun_current_piece, len(PIECES) - 1)
piece = PIECES[idx]

# â”€â”€â”€ Card de celebraciÃ³n al completar todas las piezas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if n_installed == 10:
    st.markdown("""
    <div style="background: linear-gradient(135deg, #1a0a04, #2a1a08);
                border: 2px solid #C4922A; border-radius:12px;
                padding:24px; text-align:center; margin-bottom:20px">
      <div style="font-size:3rem">ğŸ–ï¸</div>
      <h2 style="color:#C4922A">EL CASO ESTÃ CERRADO</h2>
      <p style="color:#D4CAB8; font-style:italic">
        Los diez sospechosos han sido fichados.<br>
        UBUN.IA opera al completo.<br>
        El detective puede descansar.
      </p>
      <p style="color:#9A8C78; font-size:0.8rem">
        CP25/152 Â· GSIC/EMIC Â· Universidad de Valladolid
      </p>
    </div>
    """, unsafe_allow_html=True)

# â”€â”€â”€ Dos columnas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

col_left, col_right = st.columns([0.4, 0.6])

with col_left:
    st.markdown("#### ğŸ¤– UBUN.IA â€” Pipeline del sistema")
    with st.expander("â“ Â¿QuÃ© estoy viendo aquÃ­?"):
        st.markdown("""
        <div style="color:#D4CAB8; font-family:monospace; font-size:0.85rem;
                    line-height:1.8; padding:8px">
        <p>Este es el <strong style="color:#C4922A">pipeline de GENIE Learn</strong>
        â€” el camino que sigue cada pregunta de un estudiante.</p>
        <p>Las piezas se leen de <strong>arriba a abajo</strong>:</p>
        <p>ğŸ§  <strong style="color:#C4922A">LLM</strong> genera la respuesta<br>
        â†“ recibe instrucciones del...<br>
        â¤ï¸ <strong style="color:#C4922A">Middleware</strong> que aplica las reglas del docente<br>
        â†“ que se nutre de...<br>
        ğŸ’¾ <strong style="color:#C4922A">ChromaDB</strong> con los apuntes del curso<br>
        â†“ todo corre sobre...<br>
        ğŸ¦´ <strong style="color:#C4922A">Python</strong> que conecta los mÃ³dulos<br>
        â†“ que procesa datos con...<br>
        ğŸ’ª <strong style="color:#C4922A">Pandas</strong> y los visualiza con ğŸ‘ï¸ <strong style="color:#C4922A">Plotly</strong></p>
        <p><strong style="color:#048A81">Verde</strong> = instalado Â·
        <strong style="color:#E87722">Naranja</strong> = en progreso Â·
        <strong style="color:#444">Gris</strong> = bloqueado</p>
        </div>
        """, unsafe_allow_html=True)
    render_robot(set(st.session_state.ubun_pieces_installed), piece["id"])
    st.progress(n_installed / 10.0)
    st.caption(f"{n_installed}/10 piezas instaladas")

    if n_installed <= 2:
        badge = "Inspector Novato ğŸ”"
    elif n_installed <= 5:
        badge = "Detective Senior ğŸ•µï¸"
    elif n_installed <= 8:
        badge = "Comisario ğŸ–ï¸"
    else:
        badge = "El Caso EstÃ¡ Cerrado âœ…"
    st.markdown(f'<span class="ubun-badge" style="background:{COLOR_CURRENT};color:#000;">{badge}</span>', unsafe_allow_html=True)

with col_right:
    st.markdown(f"#### âš¡ Instalando: **{piece['name']}** ({piece['tech']})")
    with st.expander("â“ Â¿CÃ³mo responder correctamente?"):
        st.markdown("""
        <div style="color:#D4CAB8; font-family:monospace; font-size:0.85rem;
                    line-height:1.8; padding:8px">
        <p>Cada pieza tiene <strong style="color:#C4922A">3 preguntas</strong>
        que escalan en dificultad:</p>
        <p>ğŸ“˜ <strong>Pregunta 1 (Bloom 2 â€” Comprender)</strong><br>
        &nbsp;&nbsp;Â¿QuÃ© es y para quÃ© sirve? ExplÃ­calo con tus palabras.</p>
        <p>ğŸ“— <strong>Pregunta 2 (Bloom 3 â€” Aplicar)</strong><br>
        &nbsp;&nbsp;Â¿CÃ³mo lo usarÃ­as en el proyecto GENIE?</p>
        <p>ğŸ“• <strong>Pregunta 3 (Bloom 4 â€” Analizar)</strong><br>
        &nbsp;&nbsp;Â¿Por quÃ© esta soluciÃ³n y no otra?</p>
        <p>âœ… Una respuesta se acepta si:<br>
        &nbsp;&nbsp;â€¢ Tiene mÃ¡s de 20 palabras<br>
        &nbsp;&nbsp;â€¢ Menciona algÃºn concepto clave de la tecnologÃ­a<br>
        &nbsp;&nbsp;â€¢ (El hint aparece si no pasa la validaciÃ³n)</p>
        </div>
        """, unsafe_allow_html=True)
    st.caption(piece["analogy"])
    st.markdown(f"""
<div style="background:#1a0a04; border:1px solid #4A3520;
            border-radius:6px; padding:12px; margin:8px 0;
            font-family:monospace; font-size:0.8rem;">
  <span style="color:#C4922A;">ğŸ“‹ FICHA POLICIAL</span><br>
  <span style="color:#9A8C78; font-style:italic;">
    {piece['ficha']}
  </span>
</div>
""", unsafe_allow_html=True)
    st.code(piece["code"], language="python")

    qs = [piece["q1"], piece["q2"], piece["q3"]]
    keywords = piece["keywords"]
    q_ok = st.session_state.ubun_questions_answered[piece["id"]]
    answers = st.session_state.ubun_answers[piece["id"]]

    bloom_captions = [
        "ğŸ’¡ Bloom 2 Â· Demuestra que entiendes quÃ© es y para quÃ© existe",
        "ğŸ’¡ Bloom 3 Â· Demuestra que sabes usarlo en el proyecto real",
        "ğŸ’¡ Bloom 4 Â· Demuestra que entiendes por quÃ© esta soluciÃ³n y no otra",
    ]
    for i in range(3):
        st.markdown(f"**Pregunta {i+1}** (Bloom {i+2})")
        st.caption(bloom_captions[i])
        ans = st.text_area(f"Respuesta {i+1}", value=answers[i], key=f"ubun_a_{piece['id']}_{i}", height=80)
        if ans != answers[i]:
            st.session_state.ubun_answers[piece["id"]][i] = ans
            ok, msg = answer_acceptable(ans, keywords)
            if ok:
                st.session_state.ubun_questions_answered[piece["id"]][i] = True
                st.success(msg)
            else:
                nw = len(ans.split())
                st.warning(f"""
**Respuesta demasiado corta o sin el concepto clave.**

Para que se acepte necesitas:
- MÃ¡s de 20 palabras ({'âœ…' if nw >= 20 else f'âŒ tienes {nw}'})
- Mencionar alguna de estas palabras: `{', '.join(piece['keywords'][:3])}`

ğŸ’¡ *Pista: Incluye alguno de los conceptos clave y desarrolla la idea con mÃ¡s de 20 palabras.*
""")
        elif q_ok[i]:
            st.success("âœ… Respuesta aceptada.")

    if all(q_ok) and piece["id"] not in st.session_state.ubun_pieces_installed:
        if st.button("ğŸ” FICHAR SOSPECHOSO", type="primary", key="ubun_install"):
            st.session_state.ubun_pieces_installed.append(piece["id"])
            st.session_state.ubun_current_piece = min(idx + 1, len(PIECES) - 1)
            st.session_state.ubun_just_installed = piece["id"]
            mensajes = {
                "python": "ğŸ” Python arrestado e integrado. El esqueleto sostiene.",
                "middleware": "ğŸ” Middleware identificado. El corazÃ³n late. Las reglas se aplican.",
                "llm": "ğŸ” LLM detenido. El cerebro habla. Pero obedece al corazÃ³n.",
                "pandas": "ğŸ” Pandas incorporado. Los mÃºsculos procesan. Los datos fluyen.",
                "plotly": "ğŸ” Plotly capturado. Los ojos ven. El docente comprende.",
                "rag": "ğŸ” ChromaDB localizado. La memoria recuerda. Sin inventar.",
                "postgresql": "ğŸ” PostgreSQL fichado. El archivero guarda. Para siempre.",
                "fastapi": "ğŸ” FastAPI interceptado. Los mÃ³dulos se comunican.",
                "streamlit": "ğŸ” Streamlit detenido. La interfaz responde. El humano toca.",
                "git": "ğŸ” Git arrestado. El caso queda documentado. Todo el caso.",
            }
            st.success(mensajes.get(piece["id"], "ğŸ” Pieza instalada."))
            st.balloons()
            st.rerun()

# â”€â”€â”€ SecciÃ³n inferior: Esta pieza en acciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if st.session_state.ubun_just_installed or n_installed > 0:
    show_id = st.session_state.ubun_just_installed or st.session_state.ubun_pieces_installed[-1]
    p_show = next(x for x in PIECES if x["id"] == show_id)
    st.markdown("---")
    st.markdown("#### Lo que GENIE Learn hace con esto")
    st.markdown(f'<div class="ubun-card"><strong>{p_show["name"]} ({p_show["tech"]}) en acciÃ³n en el dashboard:</strong><br>{p_show["in_action"]}</div>', unsafe_allow_html=True)
    if st.session_state.ubun_just_installed:
        st.session_state.ubun_just_installed = None

st.markdown('<p class="footer-ubun">UBUN.IA Â· Ganador Hack4Edu 2025 Â· CP25/152 GSIC/EMIC</p>', unsafe_allow_html=True)
