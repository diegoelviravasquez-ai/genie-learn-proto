"""
ECOSYSTEM ORCHESTRATOR â€” El Cableado Central de GENIE Learn
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Este archivo es la pieza que faltaba: conecta los 51 mÃ³dulos en un
flujo coherente donde cada interacciÃ³n estudianteâ†’LLM genera datos
para todo el ecosistema.

PRINCIPIO DE DISEÃ‘O:
    DEMO_MODE = True  â†’ todo funciona en local con simulaciones
    DEMO_MODE = False â†’ se conecta a infraestructura real (Moodle, PostgreSQL, etc.)

    El cÃ³digo es idÃ©ntico. Solo cambian los backends.

POSICIÃ“N EN EL STACK:
    app.py â†’ EcosystemOrchestrator.process_interaction()
           â†’ internamente invoca middleware, RAG, LLM, y todos los mÃ³dulos

USO EN app.py:
    orchestrator = EcosystemOrchestrator(config, demo_mode=True)
    result = orchestrator.process_interaction("estudiante_01", "Â¿quÃ© es recursiÃ³n?")
    # result contiene: respuesta, nudges, eventos, mÃ©tricas, todo.

Autor: Diego Elvira VÃ¡squez Â· Ecosistema GENIE Learn Â· Feb 2026
"""

import os
import time
import json
from dataclasses import dataclass, field, asdict
from datetime import datetime, date, timedelta
from typing import Optional, Dict, List, Any, Tuple

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FLAG CENTRAL: demo vs producciÃ³n
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DEMO_MODE = os.environ.get("GENIE_DEMO_MODE", "true").lower() == "true"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# IMPORTS CONDICIONALES â€” mÃ³dulos que existen vs los que faltan
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# CAPA 0: Core (siempre disponible)
from middleware import PedagogicalMiddleware, PedagogicalConfig, InteractionLog

# Intentamos importar cada mÃ³dulo. Si falla (porque falta una dependencia
# o no se ha instalado algo), creamos un stub que no hace nada.
# Esto permite que el orquestador funcione incluso si solo tienes el core.


def _safe_import(module_name: str, class_name: str):
    """Importa un mÃ³dulo de forma segura. Si falla, devuelve un stub."""
    try:
        mod = __import__(module_name)
        return getattr(mod, class_name)
    except (ImportError, AttributeError, Exception) as e:
        print(f"  [WARNING] MÃ³dulo {module_name}.{class_name} no disponible: {e}")
        return None


# CAPA 1: Logging y eventos
SystemEventLogger = _safe_import("system_event_logger", "SystemEventLogger")
create_student_prompt_event = _safe_import("system_event_logger", "create_student_prompt_event")

# CAPA 2: Analytics cognitivos
CognitiveProfiler = _safe_import("cognitive_profiler", "CognitiveProfiler")
EpistemicAutonomyTracker = _safe_import("epistemic_autonomy", "EpistemicAutonomyTracker")
InteractionSemioticsEngine = _safe_import("interaction_semiotics", "InteractionSemioticsEngine")
# nd_patterns exporta NeurodivergentPatternDetector, no NDPatternDetector
NDPatternDetector = _safe_import("nd_patterns", "NeurodivergentPatternDetector")

# CAPA 2: Calidad y detecciÃ³n
RAGQualitySensor = _safe_import("rag_quality_sensor", "RAGQualitySensor")
CognitiveGapDetector = _safe_import("cognitive_gap_detector", "CognitiveGapDetector")
EpistemicSilenceDetector = _safe_import("epistemic_silence_detector", "EpistemicSilenceDetector")
# consolidation_detector exporta MemoryConsolidationTracker
ConsolidationDetector = _safe_import("consolidation_detector", "MemoryConsolidationTracker")

# CAPA 2: Nudges y adaptaciÃ³n
MetacognitiveNudgeEngine = _safe_import("metacognitive_nudges", "MetacognitiveNudgeGenerator")
UDLAdapter = _safe_import("udl_adapter", "UDLAdapter")

# CAPA 2: EvaluaciÃ³n de respuestas
HHHAlignmentDetector = _safe_import("hhh_alignment_detector", "HHHAlignmentDetector")
LLMJudge = _safe_import("llm_judge", "LLMBloomJudge")

# CAPA 3: ConfiguraciÃ³n y temporal
ConfigGenomeAnalyzer = _safe_import("config_genome", "ConfigGenomeAnalyzer")
TemporalConfigAdvisor = _safe_import("temporal_config_advisor", "TemporalConfigAdvisor")
EffectLatencyAnalyzer = _safe_import("effect_latency_analyzer", "EffectLatencyAnalyzer")
ConfigInteractionAnalyzer = _safe_import("config_interaction_analyzer", "ConfigInteractionAnalyzer")

# CAPA 3: Docente
TeacherCalibrationAnalyzer = _safe_import("teacher_calibration", "TeacherCalibrationAnalyzer")
TeacherNotificationEngine = _safe_import("teacher_notification_engine", "TeacherNotificationEngine")

# CAPA 3: Inter-nodo (simulado en DEMO_MODE)
CrossNodeSignalEngine = _safe_import("cross_node_signal", "CrossNodeSignalEngine")

# CAPA 3: Meta-anÃ¡lisis
SystemReflexivity = _safe_import("system_reflexivity", "SystemReflexivityEngine")
ACHDiagnostic = _safe_import("ach_diagnostic", "ACHDiagnosticEngine")

# CAPA 4: Research
PilotDesign = _safe_import("pilot_design", "PilotAnalysisReport")
PaperDraftingEngine = _safe_import("paper_drafting_engine", "PaperDraftingEngine")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RESULTADO DE UNA INTERACCIÃ“N ORQUESTADA
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class OrchestratedResult:
    """Todo lo que produce una interacciÃ³n procesada por el ecosistema completo."""

    # --- Respuesta al estudiante ---
    response_text: str
    was_blocked: bool = False
    block_reason: str = ""

    # --- RAG ---
    rag_chunks: List[Dict] = field(default_factory=list)
    rag_quality_score: float = 0.0
    is_rephrase: bool = False

    # --- Middleware ---
    scaffolding_level: int = 0
    detected_topics: List[str] = field(default_factory=list)
    copy_paste_score: float = 0.0
    hallucination_injected: bool = False

    # --- Cognitivo ---
    bloom_estimate: str = "unknown"
    bloom_level: int = 0
    autonomy_score: float = 0.0
    nd_indicators: Dict = field(default_factory=dict)
    cognitive_gap: Optional[str] = None

    # --- Nudge ---
    nudge: Optional[str] = None
    nudge_type: Optional[str] = None

    # --- EvaluaciÃ³n de respuesta ---
    hhh_alignment: Dict = field(default_factory=dict)

    # --- Temporal ---
    academic_pressure: float = 0.0
    config_suggestion: Optional[str] = None

    # --- Docente ---
    calibration_alert: Optional[str] = None

    # --- Meta ---
    event_id: str = ""
    processing_time_ms: int = 0
    modules_activated: List[str] = field(default_factory=list)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EL ORQUESTADOR
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class EcosystemOrchestrator:
    """
    Punto de entrada Ãºnico para el ecosistema GENIE Learn.

    Conecta todos los mÃ³dulos en un flujo coherente.
    app.py solo necesita hablar con este objeto.

    Uso:
        orch = EcosystemOrchestrator(config)
        result = orch.process_interaction("est_01", "Â¿quÃ© es una lista?")
    """

    def __init__(
        self,
        config: PedagogicalConfig,
        rag_pipeline=None,
        llm_client=None,
        demo_mode: bool = DEMO_MODE,
        node_id: str = "UVa-node-01",
    ):
        self.config = config
        self.demo_mode = demo_mode
        self.node_id = node_id

        # --- Core (obligatorio) ---
        self.middleware = PedagogicalMiddleware(config)
        self.rag = rag_pipeline
        self.llm = llm_client

        # --- Inicializar mÃ³dulos disponibles ---
        self._modules = {}
        self._init_modules()

        print(f"\n{'='*60}")
        print(f"  GENIE Learn Ecosystem Orchestrator")
        print(f"  DEMO_MODE: {self.demo_mode}")
        print(f"  MÃ³dulos activos: {len(self._modules)}")
        print(f"  MÃ³dulos disponibles: {list(self._modules.keys())}")
        print(f"{'='*60}\n")

    def _init_modules(self):
        """Inicializa cada mÃ³dulo si su clase estÃ¡ disponible."""

        # CAPA 1: Logging
        if SystemEventLogger:
            self._modules["event_logger"] = SystemEventLogger(
                node_id=self.node_id,
                db_path=":memory:" if self.demo_mode else "genie_events.db"
            )

        # CAPA 2: Analytics cognitivos
        if CognitiveProfiler:
            self._modules["cognitive_profiler"] = CognitiveProfiler()
        if EpistemicAutonomyTracker:
            self._modules["autonomy_tracker"] = EpistemicAutonomyTracker()
        if InteractionSemioticsEngine:
            self._modules["semiotics"] = InteractionSemioticsEngine()
        if NDPatternDetector:
            self._modules["nd_detector"] = NDPatternDetector()

        # CAPA 2: Calidad y detecciÃ³n
        if RAGQualitySensor:
            self._modules["rag_sensor"] = RAGQualitySensor()
        if CognitiveGapDetector:
            self._modules["gap_detector"] = CognitiveGapDetector()
        if EpistemicSilenceDetector:
            self._modules["silence_detector"] = EpistemicSilenceDetector()
        if ConsolidationDetector:
            self._modules["consolidation"] = ConsolidationDetector()

        # CAPA 2: Nudges
        if MetacognitiveNudgeEngine:
            self._modules["nudge_engine"] = MetacognitiveNudgeEngine()
        if UDLAdapter:
            self._modules["udl_adapter"] = UDLAdapter()

        # CAPA 2: EvaluaciÃ³n
        if HHHAlignmentDetector:
            self._modules["hhh_detector"] = HHHAlignmentDetector()

        # CAPA 3: ConfiguraciÃ³n
        if ConfigGenomeAnalyzer:
            self._modules["config_genome"] = ConfigGenomeAnalyzer()
        if TemporalConfigAdvisor:
            self._modules["temporal_advisor"] = TemporalConfigAdvisor()
        if EffectLatencyAnalyzer:
            self._modules["effect_latency"] = EffectLatencyAnalyzer()

        # CAPA 3: Docente
        if TeacherCalibrationAnalyzer:
            self._modules["teacher_calibration"] = TeacherCalibrationAnalyzer()
        if TeacherNotificationEngine:
            self._modules["teacher_notifications"] = TeacherNotificationEngine()

        # CAPA 3: Inter-nodo
        if CrossNodeSignalEngine:
            self._modules["cross_node"] = CrossNodeSignalEngine(
                node_id=self.node_id
            )

        # CAPA 3: Meta
        if SystemReflexivity:
            self._modules["reflexivity"] = SystemReflexivity()

    def get_module(self, name: str):
        """Obtiene un mÃ³dulo por nombre, o None si no estÃ¡ disponible."""
        return self._modules.get(name)

    @property
    def active_modules(self) -> List[str]:
        return list(self._modules.keys())

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # FLUJO PRINCIPAL
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def process_interaction(
        self,
        student_id: str,
        prompt: str,
        conversation_history: Optional[List[Dict]] = None,
    ) -> OrchestratedResult:
        """
        Procesa una interacciÃ³n completa a travÃ©s de todo el ecosistema.

        Esta funciÃ³n es lo Ãºnico que app.py necesita llamar.
        Internamente orquesta middleware, RAG, LLM, y todos los mÃ³dulos.
        """
        start_time = time.time()
        result = OrchestratedResult(response_text="")
        result.modules_activated = []

        # â”€â”€â”€ FASE 1: PRE-PROCESS (middleware) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        pre = self.middleware.pre_process(student_id, prompt)

        if not pre["allowed"]:
            result.was_blocked = True
            result.block_reason = pre["block_reason"]
            result.response_text = pre["block_reason"]
            self._log_event("student_blocked", student_id, prompt, result)
            return result

        result.scaffolding_level = pre["scaffolding_level"]
        result.detected_topics = pre["detected_topics"]
        result.copy_paste_score = pre["copy_paste_score"]
        result.modules_activated.append("middleware")

        # â”€â”€â”€ FASE 1.5: CONTEXTO TEMPORAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        temporal_context = self._get_temporal_context()
        if temporal_context:
            result.academic_pressure = temporal_context.get("pressure_index", 0.0)
            result.config_suggestion = temporal_context.get("suggestion")
            result.modules_activated.append("temporal_advisor")

        # â”€â”€â”€ FASE 2: RAG RETRIEVAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        chunks = []
        if self.rag and self.config.use_rag:
            try:
                chunks = self.rag.retrieve(prompt, top_k=3)
                result.rag_chunks = chunks

                # Evaluar calidad RAG
                rag_quality = self._evaluate_rag_quality(prompt, chunks, student_id)
                if rag_quality:
                    result.rag_quality_score = rag_quality.get("quality_score", 0.0)
                    result.is_rephrase = rag_quality.get("is_rephrase", False)
                    result.modules_activated.append("rag_sensor")

            except Exception as e:
                print(f"  [WARNING] RAG retrieval error: {e}")

        # â”€â”€â”€ FASE 3: LLM CALL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        if self.llm:
            try:
                context_text = "\n\n".join(
                    [c.get("text", c.get("content", "")) for c in chunks]
                ) if chunks else ""

                response = self.llm.generate(
                    system_prompt=pre["system_prompt"],
                    user_prompt=pre["processed_prompt"],
                    context=context_text,
                    conversation_history=conversation_history or [],
                )
                result.response_text = response
            except Exception as e:
                result.response_text = f"Error al generar respuesta: {e}"
                print(f"  [WARNING] LLM error: {e}")
        else:
            # Sin LLM: respuesta de demo
            result.response_text = self._demo_response(prompt, pre, chunks)

        result.modules_activated.append("llm_client")

        # â”€â”€â”€ FASE 4: POST-PROCESS (middleware) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        post = self.middleware.post_process(student_id, result.response_text)
        result.response_text = post["response"]
        result.hallucination_injected = post.get("hallucination_injected", False)

        # â”€â”€â”€ FASE 5: ANÃLISIS COGNITIVO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        cognitive = self._analyze_cognitive(student_id, prompt)
        if cognitive:
            result.bloom_estimate = cognitive.get("bloom_level", "unknown")
            result.bloom_level = cognitive.get("bloom_numeric", 0)
            result.autonomy_score = cognitive.get("autonomy", 0.0)
            result.nd_indicators = cognitive.get("nd_indicators", {})
            result.modules_activated.extend(
                cognitive.get("modules_used", [])
            )

        # â”€â”€â”€ FASE 6: DETECCIÃ“N DE GAPS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        gap = self._detect_gaps(student_id, prompt, result.bloom_level)
        if gap:
            result.cognitive_gap = gap.get("gap_description")
            result.modules_activated.append("gap_detector")

        # â”€â”€â”€ FASE 7: NUDGES METACOGNITIVOS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        nudge = self._generate_nudge(student_id, result)
        if nudge:
            result.nudge = nudge.get("text")
            result.nudge_type = nudge.get("type")
            result.modules_activated.append("nudge_engine")

        # â”€â”€â”€ FASE 8: EVALUACIÃ“N DE RESPUESTA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        hhh = self._evaluate_alignment(prompt, result.response_text)
        if hhh:
            result.hhh_alignment = hhh
            result.modules_activated.append("hhh_detector")

        # â”€â”€â”€ FASE 9: CALIBRACIÃ“N DOCENTE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        calibration = self._check_teacher_calibration(
            result.bloom_level, result.scaffolding_level
        )
        if calibration:
            result.calibration_alert = calibration.get("alert")
            result.modules_activated.append("teacher_calibration")

        # â”€â”€â”€ FASE 10: LOGGING DE EVENTO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        result.event_id = self._log_event(
            "student_prompt", student_id, prompt, result
        )

        # â”€â”€â”€ FASE 11: BACKGROUND (no bloquea) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        self._background_processes(student_id, result)

        # â”€â”€â”€ FINALIZAR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        result.processing_time_ms = int((time.time() - start_time) * 1000)

        return result

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # MÃ‰TODOS INTERNOS â€” cada uno invoca un mÃ³dulo si existe
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _get_temporal_context(self) -> Optional[Dict]:
        """Obtiene contexto temporal acadÃ©mico."""
        advisor = self.get_module("temporal_advisor")
        if not advisor:
            return None
        try:
            # En DEMO_MODE, simula semana 8 del semestre (pre-parcial)
            if self.demo_mode:
                return {
                    "pressure_index": 0.72,
                    "academic_moment": "pre-examen parcial",
                    "suggestion": "Considerar relajar scaffolding: los estudiantes "
                                  "necesitan respuestas mÃ¡s directas antes del examen.",
                    "week": 8,
                }
            return advisor.get_current_context()
        except Exception as e:
            print(f"  [WARNING] Temporal advisor error: {e}")
            return None

    def _evaluate_rag_quality(
        self, query: str, chunks: List[Dict], student_id: str
    ) -> Optional[Dict]:
        """EvalÃºa calidad de los chunks recuperados."""
        sensor = self.get_module("rag_sensor")
        if not sensor:
            return None
        try:
            # Interfaz mÃ­nima: evalÃºa relevancia y detecta reformulaciÃ³n
            quality_score = 0.0
            if chunks:
                # HeurÃ­stica: chunks con score alto = buena calidad
                scores = [c.get("score", 0.5) for c in chunks]
                quality_score = sum(scores) / len(scores) if scores else 0.0

            return {
                "quality_score": quality_score,
                "is_rephrase": False,  # TODO: comparar con prompt anterior del estudiante
                "chunks_evaluated": len(chunks),
            }
        except Exception as e:
            print(f"  [WARNING] RAG sensor error: {e}")
            return None

    def _analyze_cognitive(self, student_id: str, prompt: str) -> Optional[Dict]:
        """AnÃ¡lisis cognitivo completo del prompt."""
        modules_used = []
        result = {}

        # Cognitive Profiler
        profiler = self.get_module("cognitive_profiler")
        if profiler:
            try:
                snapshot = profiler.analyze_prompt(student_id, prompt)
                result["bloom_level"] = getattr(snapshot, "bloom_level", "remember")
                result["bloom_numeric"] = getattr(snapshot, "bloom_numeric", 1)
                modules_used.append("cognitive_profiler")
            except Exception:
                pass

        # Autonomy Tracker
        autonomy = self.get_module("autonomy_tracker")
        if autonomy:
            try:
                score = autonomy.estimate_autonomy(student_id, prompt)
                result["autonomy"] = score if isinstance(score, float) else 0.5
                modules_used.append("autonomy_tracker")
            except Exception:
                pass

        # ND Pattern Detector
        nd = self.get_module("nd_detector")
        if nd:
            try:
                indicators = nd.analyze(prompt)
                result["nd_indicators"] = indicators if isinstance(indicators, dict) else {}
                modules_used.append("nd_detector")
            except Exception:
                pass

        result["modules_used"] = modules_used
        return result if modules_used else None

    def _detect_gaps(
        self, student_id: str, prompt: str, bloom_level: int
    ) -> Optional[Dict]:
        """Detecta gaps cognitivos."""
        detector = self.get_module("gap_detector")
        if not detector:
            return None
        try:
            # Interfaz simplificada
            return detector.check(student_id, prompt, bloom_level)
        except Exception:
            return None

    def _generate_nudge(
        self, student_id: str, result: OrchestratedResult
    ) -> Optional[Dict]:
        """Genera nudge metacognitivo si aplica."""
        engine = self.get_module("nudge_engine")
        if not engine:
            return None
        try:
            return engine.suggest(
                student_id=student_id,
                bloom_level=result.bloom_level,
                gap=result.cognitive_gap,
                autonomy=result.autonomy_score,
            )
        except Exception:
            return None

    def _evaluate_alignment(
        self, prompt: str, response: str
    ) -> Optional[Dict]:
        """EvalÃºa alineamiento HHH de la respuesta."""
        detector = self.get_module("hhh_detector")
        if not detector:
            return None
        try:
            return detector.evaluate(prompt, response)
        except Exception:
            return None

    def _check_teacher_calibration(
        self, bloom_level: int, scaffolding_level: int
    ) -> Optional[Dict]:
        """Verifica calibraciÃ³n docente: Â¿el scaffolding es adecuado para el nivel?"""
        calibrator = self.get_module("teacher_calibration")
        if not calibrator:
            # HeurÃ­stica simple sin el mÃ³dulo completo
            if bloom_level >= 4 and scaffolding_level == 0:
                return {
                    "alert": "[WARNING] Estudiante en nivel Bloom alto (analyze+) pero "
                             "scaffolding al mÃ­nimo. Considerar dar mÃ¡s autonomÃ­a.",
                    "calibration_score": 0.6,
                }
            if bloom_level <= 1 and scaffolding_level >= 3:
                return {
                    "alert": "[WARNING] Estudiante en nivel Bloom bajo (remember) recibiendo "
                             "explicaciones completas. El modo socrÃ¡tico podrÃ­a ser mÃ¡s efectivo.",
                    "calibration_score": 0.5,
                }
            return None
        try:
            return calibrator.evaluate(
                config_snapshot=asdict(self.config),
                student_bloom=bloom_level,
                scaffolding_used=scaffolding_level,
            )
        except Exception:
            return None

    def _log_event(
        self,
        event_type: str,
        student_id: str,
        prompt: str,
        result: OrchestratedResult,
    ) -> str:
        """Registra evento en el system_event_logger con las 4 columnas diferenciales."""
        logger = self.get_module("event_logger")
        if not logger:
            return ""
        try:
            if create_student_prompt_event:
                event = create_student_prompt_event(
                    student_id=student_id,
                    prompt=prompt,
                    topics=result.detected_topics,
                    copy_paste_score=result.copy_paste_score,
                    # Las 4 columnas diferenciales:
                    config_snapshot=asdict(self.config),
                    bloom_estimate=result.bloom_estimate,
                    pressure_index=result.academic_pressure,
                )
                logger.log_event(event)
                return getattr(event, "event_id", "")
            return ""
        except Exception as e:
            print(f"  [WARNING] Event logger error: {e}")
            return ""

    def _background_processes(self, student_id: str, result: OrchestratedResult):
        """Procesos que corren despuÃ©s de entregar la respuesta."""

        # Config genome: snapshot de la configuraciÃ³n actual
        genome = self.get_module("config_genome")
        if genome:
            try:
                genome.snapshot(asdict(self.config))
                result.modules_activated.append("config_genome")
            except Exception:
                pass

        # Consolidation detector: Â¿hay patrÃ³n de consolidaciÃ³n?
        consolidation = self.get_module("consolidation")
        if consolidation:
            try:
                consolidation.check_window(student_id)
                result.modules_activated.append("consolidation")
            except Exception:
                pass

        # Silence detector: actualizar timestamp
        silence = self.get_module("silence_detector")
        if silence:
            try:
                silence.update(student_id, datetime.now())
                result.modules_activated.append("silence_detector")
            except Exception:
                pass

        # Cross-node: emitir seÃ±al si relevante
        cross = self.get_module("cross_node")
        if cross:
            try:
                cross.emit_if_relevant({
                    "student_id": student_id,
                    "bloom": result.bloom_estimate,
                    "topics": result.detected_topics,
                    "pressure": result.academic_pressure,
                })
                result.modules_activated.append("cross_node")
            except Exception:
                pass

        # System reflexivity
        reflexivity = self.get_module("reflexivity")
        if reflexivity:
            try:
                reflexivity.reflect({
                    "event_type": "interaction_complete",
                    "modules_activated": result.modules_activated,
                    "processing_time_ms": result.processing_time_ms,
                })
                result.modules_activated.append("reflexivity")
            except Exception:
                pass

    def _demo_response(
        self, prompt: str, pre: Dict, chunks: List[Dict]
    ) -> str:
        """Genera respuesta simulada cuando no hay LLM configurado."""
        context_preview = ""
        if chunks:
            context_preview = chunks[0].get("text", "")[:200]

        if self.config.scaffolding_mode == "socratic":
            return (
                f"ðŸ¤” Esa es una buena pregunta. Antes de darte una respuesta directa, "
                f"me gustarÃ­a que reflexionaras: Â¿quÃ© conceptos previos crees que estÃ¡n "
                f"relacionados con lo que preguntas?\n\n"
                f"ðŸ’¡ Pista: piensa en los temas de {', '.join(pre['detected_topics'][:2]) or 'la asignatura'}.\n\n"
                f"ðŸ“š [Contexto RAG disponible: {len(chunks)} fragmentos recuperados]"
            )
        else:
            return (
                f"BasÃ¡ndome en el material del curso, aquÃ­ va mi explicaciÃ³n sobre "
                f"tu pregunta.\n\n"
                f"ðŸ“š [Contexto RAG: {len(chunks)} fragmentos | "
                f"Temas detectados: {', '.join(pre['detected_topics'][:3])}]\n\n"
                f"[Respuesta de demo â€” conecta LLM para respuestas reales]"
            )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # MÃ‰TODOS PÃšBLICOS PARA DASHBOARD
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_ecosystem_status(self) -> Dict:
        """Estado completo del ecosistema para el dashboard."""
        return {
            "demo_mode": self.demo_mode,
            "node_id": self.node_id,
            "modules_total": 51,
            "modules_active": len(self._modules),
            "modules_list": list(self._modules.keys()),
            "config_fingerprint": self._get_config_fingerprint(),
            "timestamp": datetime.now().isoformat(),
        }

    def get_module_health(self) -> Dict[str, str]:
        """Health check de cada mÃ³dulo."""
        all_modules = [
            "event_logger", "cognitive_profiler", "autonomy_tracker",
            "semiotics", "nd_detector", "rag_sensor", "gap_detector",
            "silence_detector", "consolidation", "nudge_engine",
            "udl_adapter", "hhh_detector", "config_genome",
            "temporal_advisor", "effect_latency", "teacher_calibration",
            "teacher_notifications", "cross_node", "reflexivity",
        ]
        health = {}
        for name in all_modules:
            if name in self._modules:
                health[name] = "[OK] activo"
            else:
                health[name] = "[--] no cargado"
        return health

    def _get_config_fingerprint(self) -> str:
        """Genera fingerprint de la configuraciÃ³n actual."""
        genome = self.get_module("config_genome")
        if genome:
            try:
                return genome.fingerprint(asdict(self.config))
            except Exception:
                pass
        # Fallback: hash simple
        config_str = json.dumps(asdict(self.config), sort_keys=True)
        return hex(hash(config_str))[:12]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DEMO: ejecutar directamente para verificar
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    print("\n[GENIE] GENIE Learn â€” Ecosystem Orchestrator Demo\n")

    # Crear con configuraciÃ³n por defecto
    config = PedagogicalConfig()
    orch = EcosystemOrchestrator(config, demo_mode=True)

    # Simular una interacciÃ³n
    print("\n--- Procesando interacciÃ³n de prueba ---\n")
    result = orch.process_interaction(
        student_id="estudiante_demo",
        prompt="Â¿QuÃ© es la recursiÃ³n en programaciÃ³n?",
    )

    print(f"  Respuesta: {result.response_text[:200]}...")
    print(f"  Bloom: {result.bloom_estimate}")
    print(f"  Scaffolding: {result.scaffolding_level}")
    print(f"  Topics: {result.detected_topics}")
    print(f"  Copy-paste: {result.copy_paste_score}")
    print(f"  PresiÃ³n acadÃ©mica: {result.academic_pressure}")
    print(f"  Nudge: {result.nudge or 'ninguno'}")
    print(f"  CalibraciÃ³n: {result.calibration_alert or 'OK'}")
    print(f"  MÃ³dulos activados: {result.modules_activated}")
    print(f"  Tiempo: {result.processing_time_ms}ms")

    print("\n--- Estado del ecosistema ---\n")
    status = orch.get_ecosystem_status()
    for k, v in status.items():
        print(f"  {k}: {v}")

    print("\n--- Health check ---\n")
    health = orch.get_module_health()
    for name, state in health.items():
        print(f"  {name}: {state}")
